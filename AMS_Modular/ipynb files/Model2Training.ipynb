{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86e48b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Pandas version: 2.2.2\n",
      "TensorFlow version: 2.17.0\n",
      "NumPy version: 1.23.5\n",
      "scikit-learn version: 1.4.2\n",
      "Transformers version: 4.44.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "# Required Libraries and Their Purposes:\n",
    "# - pandas: Used for data manipulation and analysis, particularly for reading and writing CSV files.\n",
    "# - transformers: Provides the BERT model and tokenizer for sequence classification tasks.\n",
    "# - tensorflow: Used for model training, including defining, compiling, and fitting neural networks.\n",
    "# - numpy: Supports numerical operations, such as manipulating arrays and tensors.\n",
    "# - sklearn: Used for splitting data into training and validation sets.\n",
    "\n",
    "# Check and Print Library Versions\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "# Required pip installs for the project (Uncomment to install)\n",
    "\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install tensorflow==2.17.0\n",
    "# !pip install numpy==1.23.5\n",
    "# !pip install scikit-learn==1.4.2\n",
    "# !pip install transformers==4.44.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd2cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: data/training/metricTraining\\metric1.csv\n",
      "Loading file: data/training/metricTraining\\metric2.csv\n",
      "Loading file: data/training/metricTraining\\metric3.csv\n",
      "Loading file: data/training/metricTraining\\metric4.csv\n",
      "Loading file: data/training/metricTraining\\metric5.csv\n",
      "Loading file: data/training/metricTraining\\metric6.csv\n",
      "All files concatenated into one DataFrame.\n",
      "Model loaded successfully.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:641: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_for_sequence_classification_7/bert/pooler/dense/kernel:0', 'tf_bert_for_sequence_classification_7/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "14/14 [==============================] - ETA: 0s - loss: 1.8604 - category_output_loss: 0.8013 - thank_customer_loss: 0.0749 - introduce_self_loss: 0.1333 - ask_reason_loss: 0.0528 - ask_accurate_loss: 0.1530 - ask_permission_loss: 0.1216 - resolve_issue_loss: 0.2656 - offer_assistance_loss: 0.0595 - thank_again_loss: 0.1128 - farewell_loss: 0.0856 - category_output_accuracy: 0.7752 - thank_customer_accuracy: 0.9862 - introduce_self_accuracy: 0.9771 - ask_reason_accuracy: 0.9862 - ask_accurate_accuracy: 0.9679 - ask_permission_accuracy: 0.9633 - resolve_issue_accuracy: 0.8899 - offer_assistance_accuracy: 0.9817 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 88s 4s/step - loss: 1.8604 - category_output_loss: 0.8013 - thank_customer_loss: 0.0749 - introduce_self_loss: 0.1333 - ask_reason_loss: 0.0528 - ask_accurate_loss: 0.1530 - ask_permission_loss: 0.1216 - resolve_issue_loss: 0.2656 - offer_assistance_loss: 0.0595 - thank_again_loss: 0.1128 - farewell_loss: 0.0856 - category_output_accuracy: 0.7752 - thank_customer_accuracy: 0.9862 - introduce_self_accuracy: 0.9771 - ask_reason_accuracy: 0.9862 - ask_accurate_accuracy: 0.9679 - ask_permission_accuracy: 0.9633 - resolve_issue_accuracy: 0.8899 - offer_assistance_accuracy: 0.9817 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6921 - category_output_loss: 0.2997 - thank_customer_loss: 0.0362 - introduce_self_loss: 0.0515 - ask_reason_loss: 0.0314 - ask_accurate_loss: 0.0428 - ask_permission_loss: 0.0738 - resolve_issue_loss: 0.0695 - offer_assistance_loss: 0.0113 - thank_again_loss: 0.0548 - farewell_loss: 0.0210 - category_output_accuracy: 0.9174 - thank_customer_accuracy: 0.9862 - introduce_self_accuracy: 0.9771 - ask_reason_accuracy: 0.9954 - ask_accurate_accuracy: 0.9817 - ask_permission_accuracy: 0.9679 - resolve_issue_accuracy: 0.9817 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.6921 - category_output_loss: 0.2997 - thank_customer_loss: 0.0362 - introduce_self_loss: 0.0515 - ask_reason_loss: 0.0314 - ask_accurate_loss: 0.0428 - ask_permission_loss: 0.0738 - resolve_issue_loss: 0.0695 - offer_assistance_loss: 0.0113 - thank_again_loss: 0.0548 - farewell_loss: 0.0210 - category_output_accuracy: 0.9174 - thank_customer_accuracy: 0.9862 - introduce_self_accuracy: 0.9771 - ask_reason_accuracy: 0.9954 - ask_accurate_accuracy: 0.9817 - ask_permission_accuracy: 0.9679 - resolve_issue_accuracy: 0.9817 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.2798 - category_output_loss: 0.1180 - thank_customer_loss: 0.0126 - introduce_self_loss: 0.0251 - ask_reason_loss: 0.0160 - ask_accurate_loss: 0.0100 - ask_permission_loss: 0.0194 - resolve_issue_loss: 0.0340 - offer_assistance_loss: 0.0024 - thank_again_loss: 0.0259 - farewell_loss: 0.0164 - category_output_accuracy: 0.9633 - thank_customer_accuracy: 0.9954 - introduce_self_accuracy: 0.9908 - ask_reason_accuracy: 0.9954 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 0.9908 - resolve_issue_accuracy: 0.9908 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.2798 - category_output_loss: 0.1180 - thank_customer_loss: 0.0126 - introduce_self_loss: 0.0251 - ask_reason_loss: 0.0160 - ask_accurate_loss: 0.0100 - ask_permission_loss: 0.0194 - resolve_issue_loss: 0.0340 - offer_assistance_loss: 0.0024 - thank_again_loss: 0.0259 - farewell_loss: 0.0164 - category_output_accuracy: 0.9633 - thank_customer_accuracy: 0.9954 - introduce_self_accuracy: 0.9908 - ask_reason_accuracy: 0.9954 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 0.9908 - resolve_issue_accuracy: 0.9908 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9862 - farewell_accuracy: 0.9908\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1319 - category_output_loss: 0.0655 - thank_customer_loss: 0.0060 - introduce_self_loss: 0.0087 - ask_reason_loss: 0.0073 - ask_accurate_loss: 0.0041 - ask_permission_loss: 0.0097 - resolve_issue_loss: 0.0033 - offer_assistance_loss: 0.0027 - thank_again_loss: 0.0120 - farewell_loss: 0.0126 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 0.9954WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.1319 - category_output_loss: 0.0655 - thank_customer_loss: 0.0060 - introduce_self_loss: 0.0087 - ask_reason_loss: 0.0073 - ask_accurate_loss: 0.0041 - ask_permission_loss: 0.0097 - resolve_issue_loss: 0.0033 - offer_assistance_loss: 0.0027 - thank_again_loss: 0.0120 - farewell_loss: 0.0126 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 0.9954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1317 - category_output_loss: 0.0905 - thank_customer_loss: 0.0028 - introduce_self_loss: 0.0056 - ask_reason_loss: 0.0025 - ask_accurate_loss: 0.0030 - ask_permission_loss: 0.0041 - resolve_issue_loss: 0.0036 - offer_assistance_loss: 0.0015 - thank_again_loss: 0.0041 - farewell_loss: 0.0140 - category_output_accuracy: 0.9725 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 0.9954WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 60s 4s/step - loss: 0.1317 - category_output_loss: 0.0905 - thank_customer_loss: 0.0028 - introduce_self_loss: 0.0056 - ask_reason_loss: 0.0025 - ask_accurate_loss: 0.0030 - ask_permission_loss: 0.0041 - resolve_issue_loss: 0.0036 - offer_assistance_loss: 0.0015 - thank_again_loss: 0.0041 - farewell_loss: 0.0140 - category_output_accuracy: 0.9725 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 0.9954\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.1265 - category_output_loss: 0.0846 - thank_customer_loss: 0.0018 - introduce_self_loss: 0.0041 - ask_reason_loss: 0.0062 - ask_accurate_loss: 0.0028 - ask_permission_loss: 0.0031 - resolve_issue_loss: 0.0029 - offer_assistance_loss: 0.0017 - thank_again_loss: 0.0128 - farewell_loss: 0.0065 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9954 - farewell_accuracy: 0.9954WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 62s 4s/step - loss: 0.1265 - category_output_loss: 0.0846 - thank_customer_loss: 0.0018 - introduce_self_loss: 0.0041 - ask_reason_loss: 0.0062 - ask_accurate_loss: 0.0028 - ask_permission_loss: 0.0031 - resolve_issue_loss: 0.0029 - offer_assistance_loss: 0.0017 - thank_again_loss: 0.0128 - farewell_loss: 0.0065 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 0.9954 - farewell_accuracy: 0.9954\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0678 - category_output_loss: 0.0446 - thank_customer_loss: 0.0020 - introduce_self_loss: 0.0024 - ask_reason_loss: 0.0044 - ask_accurate_loss: 0.0022 - ask_permission_loss: 0.0031 - resolve_issue_loss: 0.0020 - offer_assistance_loss: 0.0017 - thank_again_loss: 0.0036 - farewell_loss: 0.0018 - category_output_accuracy: 0.9908 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.0678 - category_output_loss: 0.0446 - thank_customer_loss: 0.0020 - introduce_self_loss: 0.0024 - ask_reason_loss: 0.0044 - ask_accurate_loss: 0.0022 - ask_permission_loss: 0.0031 - resolve_issue_loss: 0.0020 - offer_assistance_loss: 0.0017 - thank_again_loss: 0.0036 - farewell_loss: 0.0018 - category_output_accuracy: 0.9908 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0592 - category_output_loss: 0.0415 - thank_customer_loss: 0.0017 - introduce_self_loss: 0.0038 - ask_reason_loss: 0.0018 - ask_accurate_loss: 0.0011 - ask_permission_loss: 0.0029 - resolve_issue_loss: 0.0016 - offer_assistance_loss: 9.9130e-04 - thank_again_loss: 0.0025 - farewell_loss: 0.0013 - category_output_accuracy: 0.9862 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.0592 - category_output_loss: 0.0415 - thank_customer_loss: 0.0017 - introduce_self_loss: 0.0038 - ask_reason_loss: 0.0018 - ask_accurate_loss: 0.0011 - ask_permission_loss: 0.0029 - resolve_issue_loss: 0.0016 - offer_assistance_loss: 9.9130e-04 - thank_again_loss: 0.0025 - farewell_loss: 0.0013 - category_output_accuracy: 0.9862 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0620 - category_output_loss: 0.0485 - thank_customer_loss: 0.0016 - introduce_self_loss: 0.0019 - ask_reason_loss: 0.0015 - ask_accurate_loss: 0.0018 - ask_permission_loss: 0.0018 - resolve_issue_loss: 0.0011 - offer_assistance_loss: 7.7066e-04 - thank_again_loss: 0.0019 - farewell_loss: 0.0010 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000   WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 61s 4s/step - loss: 0.0620 - category_output_loss: 0.0485 - thank_customer_loss: 0.0016 - introduce_self_loss: 0.0019 - ask_reason_loss: 0.0015 - ask_accurate_loss: 0.0018 - ask_permission_loss: 0.0018 - resolve_issue_loss: 0.0011 - offer_assistance_loss: 7.7066e-04 - thank_again_loss: 0.0019 - farewell_loss: 0.0010 - category_output_accuracy: 0.9817 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.0569 - category_output_loss: 0.0440 - thank_customer_loss: 0.0011 - introduce_self_loss: 0.0026 - ask_reason_loss: 0.0014 - ask_accurate_loss: 0.0014 - ask_permission_loss: 0.0011 - resolve_issue_loss: 0.0014 - offer_assistance_loss: 8.7755e-04 - thank_again_loss: 0.0020 - farewell_loss: 9.8420e-04 - category_output_accuracy: 0.9725 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000   WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,category_output_loss,thank_customer_loss,introduce_self_loss,ask_reason_loss,ask_accurate_loss,ask_permission_loss,resolve_issue_loss,offer_assistance_loss,thank_again_loss,farewell_loss,category_output_accuracy,thank_customer_accuracy,introduce_self_accuracy,ask_reason_accuracy,ask_accurate_accuracy,ask_permission_accuracy,resolve_issue_accuracy,offer_assistance_accuracy,thank_again_accuracy,farewell_accuracy\n",
      "14/14 [==============================] - 61s 4s/step - loss: 0.0569 - category_output_loss: 0.0440 - thank_customer_loss: 0.0011 - introduce_self_loss: 0.0026 - ask_reason_loss: 0.0014 - ask_accurate_loss: 0.0014 - ask_permission_loss: 0.0011 - resolve_issue_loss: 0.0014 - offer_assistance_loss: 8.7755e-04 - thank_again_loss: 0.0020 - farewell_loss: 9.8420e-04 - category_output_accuracy: 0.9725 - thank_customer_accuracy: 1.0000 - introduce_self_accuracy: 1.0000 - ask_reason_accuracy: 1.0000 - ask_accurate_accuracy: 1.0000 - ask_permission_accuracy: 1.0000 - resolve_issue_accuracy: 1.0000 - offer_assistance_accuracy: 1.0000 - thank_again_accuracy: 1.0000 - farewell_accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: sent_ana_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sent_ana_model\\assets\n",
      "C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:641: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 20s 2s/step\n",
      "Predictions saved to data/training/metricSolution\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# # This Training and Prediction Pipeline processes multiple CSV files by first \n",
    "# # concatenating them into a single DataFrame. It uses a pre-trained BERT-based \n",
    "# # model to tokenize the text and generate predictions for various submetrics \n",
    "# # (e.g., \"Thank Customer,\" \"Ask Permission,\" etc.). If no model is available, \n",
    "# # it will train a new one using the concatenated data. The model is trained with \n",
    "# # early stopping to prevent overfitting, and the predictions are then saved to a \n",
    "# # CSV file for further evaluation.\n",
    "\n",
    "# This is better for training due to a larger data pool, but if you try to \n",
    "# evaluate it, you may not get accurate %'s because with a large enough data \n",
    "# set, you will almost certainlyhave at least a 1 in every column/row which \n",
    "# will skew the go or no go scoring system\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "# # Load and preprocess the data\n",
    "# def load_data(file_path):\n",
    "#     data = pd.read_csv(file_path)\n",
    "#     return data\n",
    "\n",
    "# # Map the labels for category prediction\n",
    "# def map_labels(data):\n",
    "#     category_mapping = {'Greetings': 0, 'Account Verification': 1, 'Problem Investigation': 2, 'Closure': 3}\n",
    "#     data['Category Label'] = data['Category Truth'].apply(lambda x: category_mapping[x])\n",
    "#     return data\n",
    "\n",
    "# # Concatenate multiple files into a single DataFrame\n",
    "# def concatenate_files(file_paths):\n",
    "#     data_frames = []\n",
    "#     for file_path in file_paths:\n",
    "#         print(f\"Loading file: {file_path}\")\n",
    "#         df = pd.read_csv(file_path)\n",
    "#         data_frames.append(df)\n",
    "#     concatenated_df = pd.concat(data_frames, ignore_index=True)\n",
    "#     return concatenated_df\n",
    "\n",
    "# # Tokenize the input text using BERT tokenizer\n",
    "# def tokenize_texts(tokenizer, texts, max_length=128):\n",
    "#     return tokenizer(\n",
    "#         texts.tolist(),\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         return_tensors=\"tf\"\n",
    "#     )\n",
    "\n",
    "# # Function to load an existing model and tokenizer\n",
    "# def load_saved_model_and_tokenizer():\n",
    "#     try:\n",
    "#         # Attempt to load the Keras model\n",
    "#         model = tf.keras.models.load_model('sent_ana_model', custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification})\n",
    "#         tokenizer = BertTokenizer.from_pretrained('sent_ana_model')\n",
    "#         print(\"Model loaded successfully.\")\n",
    "#     except Exception as e:\n",
    "#         print(\"No existing model found. Building and training a new model.\")\n",
    "#         print(e)\n",
    "#         model = build_model()  # Define this function as per your architecture if needed\n",
    "#         tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     return model, tokenizer\n",
    "\n",
    "# # Compile and train the model with early stopping\n",
    "# def train_model(model, tokenizer, data, input_dir, output_dir):\n",
    "#     # Tokenize the text data\n",
    "#     inputs = tokenize_texts(tokenizer, data['Text'])\n",
    "\n",
    "#     # Define labels for each task\n",
    "#     labels_category = data['Category Label']\n",
    "#     labels_thank_customer = data['Thank Customer']\n",
    "#     labels_introduce_self = data['Introduce Self']\n",
    "#     labels_ask_reason = data['Ask Reason']\n",
    "#     labels_ask_accurate = data['Ask Accurate Details']\n",
    "#     labels_ask_permission = data['Ask Permission']\n",
    "#     labels_resolve_issue = data['Resolve Issue']\n",
    "#     labels_offer_assistance = data['Offer Assistance']\n",
    "#     labels_thank_again = data['Thank Again']\n",
    "#     labels_farewell = data['Farewell']\n",
    "\n",
    "#     # Prepare the dataset for training\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), \n",
    "#         (labels_category, labels_thank_customer, labels_introduce_self, labels_ask_reason,\n",
    "#          labels_ask_accurate, labels_ask_permission, labels_resolve_issue, labels_offer_assistance,\n",
    "#          labels_thank_again, labels_farewell)\n",
    "#     )).shuffle(100).batch(16)\n",
    "\n",
    "#     # Compile the model\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "#     loss = {\n",
    "#         'category_output': 'sparse_categorical_crossentropy',\n",
    "#         'thank_customer': 'sparse_categorical_crossentropy',\n",
    "#         'introduce_self': 'sparse_categorical_crossentropy',\n",
    "#         'ask_reason': 'sparse_categorical_crossentropy',\n",
    "#         'ask_accurate': 'sparse_categorical_crossentropy',\n",
    "#         'ask_permission': 'sparse_categorical_crossentropy',\n",
    "#         'resolve_issue': 'sparse_categorical_crossentropy',\n",
    "#         'offer_assistance': 'sparse_categorical_crossentropy',\n",
    "#         'thank_again': 'sparse_categorical_crossentropy',\n",
    "#         'farewell': 'sparse_categorical_crossentropy'\n",
    "#     }\n",
    "#     metrics = {\n",
    "#         'category_output': 'accuracy',\n",
    "#         'thank_customer': 'accuracy',\n",
    "#         'introduce_self': 'accuracy',\n",
    "#         'ask_reason': 'accuracy',\n",
    "#         'ask_accurate': 'accuracy',\n",
    "#         'ask_permission': 'accuracy',\n",
    "#         'resolve_issue': 'accuracy',\n",
    "#         'offer_assistance': 'accuracy',\n",
    "#         'thank_again': 'accuracy',\n",
    "#         'farewell': 'accuracy'\n",
    "#     }\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "#     # Add early stopping\n",
    "#     early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "#         monitor='val_loss',\n",
    "#         patience=3,  # Stop training after 3 epochs of no improvement\n",
    "#         restore_best_weights=True\n",
    "#     )\n",
    "\n",
    "#     # Train the model with early stopping\n",
    "#     model.fit(dataset, epochs=10, callbacks=[early_stopping_callback])\n",
    "\n",
    "#     # Save the model using Keras method\n",
    "#     model.save('sent_ana_model')  # Save the Keras model\n",
    "#     tokenizer.save_pretrained('sent_ana_model')  # Save the tokenizer\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Make predictions and save to CSV\n",
    "# def make_predictions(model, tokenizer, data, output_file):\n",
    "#     # Tokenize the input text\n",
    "#     inputs = tokenize_texts(tokenizer, data['Text'])\n",
    "\n",
    "#     # Predict for each output layer\n",
    "#     predictions = model.predict(dict(inputs))\n",
    "\n",
    "#     # Convert predictions to binary format for sub-criteria (0 or 1)\n",
    "#     prediction_category = np.argmax(predictions[0], axis=1)\n",
    "#     prediction_thank_customer = np.argmax(predictions[1], axis=1)\n",
    "#     prediction_introduce_self = np.argmax(predictions[2], axis=1)\n",
    "#     prediction_ask_reason = np.argmax(predictions[3], axis=1)\n",
    "#     prediction_ask_accurate = np.argmax(predictions[4], axis=1)\n",
    "#     prediction_ask_permission = np.argmax(predictions[5], axis=1)\n",
    "#     prediction_resolve_issue = np.argmax(predictions[6], axis=1)\n",
    "#     prediction_offer_assistance = np.argmax(predictions[7], axis=1)\n",
    "#     prediction_thank_again = np.argmax(predictions[8], axis=1)\n",
    "#     prediction_farewell = np.argmax(predictions[9], axis=1)\n",
    "\n",
    "#     # Add predictions to the dataframe\n",
    "#     data['Predicted Category'] = prediction_category\n",
    "#     data['Predicted Thank Customer'] = prediction_thank_customer\n",
    "#     data['Predicted Introduce Self'] = prediction_introduce_self\n",
    "#     data['Predicted Ask Reason'] = prediction_ask_reason\n",
    "#     data['Predicted Ask Accurate Details'] = prediction_ask_accurate\n",
    "#     data['Predicted Ask Permission'] = prediction_ask_permission\n",
    "#     data['Predicted Resolve Issue'] = prediction_resolve_issue\n",
    "#     data['Predicted Offer Assistance'] = prediction_offer_assistance\n",
    "#     data['Predicted Thank Again'] = prediction_thank_again\n",
    "#     data['Predicted Farewell'] = prediction_farewell\n",
    "\n",
    "#     # Save the results to a CSV file\n",
    "#     data.to_csv(output_file, index=False)\n",
    "#     print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# # Example pipeline\n",
    "# def run_pipeline():\n",
    "#     # Input and output directories for CSV files\n",
    "#     input_dir = 'data/training/metricTraining'\n",
    "#     output_dir = 'data/training/metricSolution'\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Get list of all CSV files in the input directory\n",
    "#     file_paths = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "    \n",
    "#     if not file_paths:\n",
    "#         print(f\"No CSV files found in the directory: {input_dir}\")\n",
    "#         return\n",
    "\n",
    "#     # Concatenate all files into a single DataFrame\n",
    "#     data = concatenate_files(file_paths)\n",
    "#     print(\"All files concatenated into one DataFrame.\")\n",
    "    \n",
    "#     # Try loading an existing model and tokenizer, otherwise build a new one\n",
    "#     model, tokenizer = load_saved_model_and_tokenizer()\n",
    "\n",
    "#     # Map the labels\n",
    "#     data = map_labels(data)\n",
    "\n",
    "#     # Train the model\n",
    "#     model = train_model(model, tokenizer, data, input_dir, output_dir)\n",
    "\n",
    "#     # Make predictions and save to a CSV file\n",
    "#     make_predictions(model, tokenizer, data, os.path.join(output_dir, 'predictions.csv'))\n",
    "\n",
    "# # Run the pipeline\n",
    "# run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2479f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\load.py:109: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:156: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:641: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "Predictions saved to data/training/metricSolution\\metric1_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\functional.py:641: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 186ms/step\n",
      "Predictions saved to data/training/metricSolution\\metric2_predictions.csv\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Predictions saved to data/training/metricSolution\\metric3_predictions.csv\n",
      "2/2 [==============================] - 3s 191ms/step\n",
      "Predictions saved to data/training/metricSolution\\metric4_predictions.csv\n",
      "2/2 [==============================] - 3s 636ms/step\n",
      "Predictions saved to data/training/metricSolution\\metric5_predictions.csv\n",
      "2/2 [==============================] - 4s 1s/step\n",
      "Predictions saved to data/training/metricSolution\\metric6_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2\n",
    "\n",
    "# Description\n",
    "# The Non-Concatenating Prediction Pipeline processes multiple CSV files \n",
    "# individually without combining them. It loads a pre-trained BERT-based \n",
    "# model and tokenizer, tokenizes the text from each file, and generates \n",
    "# predictions for various submetrics (like \"Thank Customer,\" \"Ask Permission,\" \n",
    "# etc.). The predictions are compared with the true labels and saved to new \n",
    "# CSV files in a specified output directory. Each file is handled separately, \n",
    "# allowing for file-specific evaluations of predictions without altering the \n",
    "# original data.\n",
    "\n",
    "# Load and preprocess the data from a single file\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Map the labels for category prediction (if needed for evaluation)\n",
    "def map_labels(data):\n",
    "    category_mapping = {'Greetings': 0, 'Account Verification': 1, 'Problem Investigation': 2, 'Closure': 3}\n",
    "    data['Category Label'] = data['Category Truth'].apply(lambda x: category_mapping[x])\n",
    "    return data\n",
    "\n",
    "# Tokenize the input text using BERT tokenizer\n",
    "def tokenize_texts(tokenizer, texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "# Function to load the saved model and tokenizer (for predictions only)\n",
    "def load_saved_model_and_tokenizer():\n",
    "    try:\n",
    "        # Load the trained Keras model\n",
    "        model = tf.keras.models.load_model('sent_ana_model', custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification})\n",
    "        tokenizer = BertTokenizer.from_pretrained('sent_ana_model')\n",
    "        print(\"Model and tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading the model or tokenizer.\")\n",
    "        raise e\n",
    "    return model, tokenizer\n",
    "\n",
    "# Make predictions for a single file and save results to CSV\n",
    "def make_predictions_for_file(model, tokenizer, data, output_file):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenize_texts(tokenizer, data['Text'])\n",
    "\n",
    "    # Predict for each output layer\n",
    "    predictions = model.predict(dict(inputs))\n",
    "\n",
    "    # Convert predictions to binary format for sub-criteria (0 or 1)\n",
    "    prediction_category = np.argmax(predictions[0], axis=1)\n",
    "    prediction_thank_customer = np.argmax(predictions[1], axis=1)\n",
    "    prediction_introduce_self = np.argmax(predictions[2], axis=1)\n",
    "    prediction_ask_reason = np.argmax(predictions[3], axis=1)\n",
    "    prediction_ask_accurate = np.argmax(predictions[4], axis=1)\n",
    "    prediction_ask_permission = np.argmax(predictions[5], axis=1)\n",
    "    prediction_resolve_issue = np.argmax(predictions[6], axis=1)\n",
    "    prediction_offer_assistance = np.argmax(predictions[7], axis=1)\n",
    "    prediction_thank_again = np.argmax(predictions[8], axis=1)\n",
    "    prediction_farewell = np.argmax(predictions[9], axis=1)\n",
    "\n",
    "    # Add predictions to the dataframe\n",
    "    data['Predicted Category'] = prediction_category\n",
    "    data['Predicted Thank Customer'] = prediction_thank_customer\n",
    "    data['Predicted Introduce Self'] = prediction_introduce_self\n",
    "    data['Predicted Ask Reason'] = prediction_ask_reason\n",
    "    data['Predicted Ask Accurate Details'] = prediction_ask_accurate\n",
    "    data['Predicted Ask Permission'] = prediction_ask_permission\n",
    "    data['Predicted Resolve Issue'] = prediction_resolve_issue\n",
    "    data['Predicted Offer Assistance'] = prediction_offer_assistance\n",
    "    data['Predicted Thank Again'] = prediction_thank_again\n",
    "    data['Predicted Farewell'] = prediction_farewell\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    data.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Prediction-only pipeline that processes each file individually\n",
    "def run_prediction_pipeline():\n",
    "    # Input and output directories for CSV files\n",
    "    input_dir = 'data/training/metricTraining'\n",
    "    output_dir = 'data/training/metricSolution'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get list of all CSV files in the input directory\n",
    "    file_paths = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"No CSV files found in the directory: {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Load the saved model and tokenizer\n",
    "    model, tokenizer = load_saved_model_and_tokenizer()\n",
    "\n",
    "    # Process each file individually\n",
    "    for file_path in file_paths:\n",
    "        # Load the data for the current file\n",
    "        data = load_data(file_path)\n",
    "        \n",
    "        # Map the labels if needed (optional, depending on your evaluation process)\n",
    "        data = map_labels(data)\n",
    "        \n",
    "        # Prepare the output file path (e.g., saving with a similar name in the output directory)\n",
    "        output_file = os.path.join(output_dir, os.path.basename(file_path).replace('.csv', '_predictions.csv'))\n",
    "        \n",
    "        # Make predictions and save the results\n",
    "        make_predictions_for_file(model, tokenizer, data, output_file)\n",
    "\n",
    "# Run the prediction-only pipeline\n",
    "run_prediction_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dc4865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved to data/training/metricEvaluation/submetricEvaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "\n",
    "# Description:\n",
    "# Evaluates the accuracy of predicted values against truth values for specific \n",
    "# submetrics across multiple CSV files in a given directory (input_dir). \n",
    "# It calculates the accuracy for each submetric (like \"Thank Customer\" \n",
    "# and \"Ask Permission\") by comparing the truth and predicted columns, \n",
    "# ignoring rows where the truth values are zero. The accuracy for each submetric \n",
    "# is computed for each file, and an overall average accuracy is calculated. \n",
    "# The results are stored in a DataFrame, with each row representing a file, and \n",
    "# a final row showing the overall average across all files.\n",
    "\n",
    "# Input directory containing all the CSV files\n",
    "input_dir = 'data/training/metricSolution'\n",
    "\n",
    "# List of submetrics\n",
    "submetrics = ['Thank Customer', 'Introduce Self', 'Ask Reason', 'Ask Accurate Details',\n",
    "              'Ask Permission', 'Resolve Issue', 'Offer Assistance', 'Thank Again', 'Farewell']\n",
    "\n",
    "# Function to calculate submetric accuracy for a single DataFrame\n",
    "def calculate_submetric_accuracy(data):\n",
    "    accuracy_results = {}\n",
    "    for submetric in submetrics:\n",
    "        truth_col = submetric\n",
    "        pred_col = f'Predicted {submetric}'\n",
    "\n",
    "        # Check if both truth and prediction columns have no 1's at all\n",
    "        if (data[truth_col].sum() == 0) and (data[pred_col].sum() == 0):\n",
    "            # If no 1's in both truth and prediction columns, accuracy is 100%\n",
    "            accuracy = 100.0\n",
    "        else:\n",
    "            # Only consider rows where the truth value is 1 (ignoring 0s)\n",
    "            valid_rows = data[data[truth_col] == 1]\n",
    "            if len(valid_rows) > 0:\n",
    "                # Calculate accuracy as percentage of correct predictions (truth == prediction)\n",
    "                accuracy = (valid_rows[truth_col] == valid_rows[pred_col]).mean() * 100\n",
    "            else:\n",
    "                accuracy = 0.0  # If no valid rows, set accuracy to 0\n",
    "\n",
    "        accuracy_results[submetric] = round(accuracy, 2)\n",
    "    return accuracy_results\n",
    "\n",
    "# Function to load CSV files from the directory and calculate accuracy for each file\n",
    "def evaluate_files(input_dir):\n",
    "    # Get list of all CSV files in the directory\n",
    "    file_paths = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "    \n",
    "    if not file_paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in the directory: {input_dir}\")\n",
    "    \n",
    "    results = {}\n",
    "    for file in file_paths:\n",
    "        file_name = os.path.basename(file).replace('.csv', '')\n",
    "        data = pd.read_csv(file)\n",
    "        results[file_name] = calculate_submetric_accuracy(data)\n",
    "    \n",
    "    # Convert results to DataFrame for better visualization\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    # Calculate the average for each row (file)\n",
    "    results_df['Average'] = results_df.mean(axis=1)\n",
    "    \n",
    "    # Add an overall average row\n",
    "    overall_average = results_df.mean(axis=0)\n",
    "    overall_average['Task'] = 'Average'\n",
    "    results_df.loc['Average'] = overall_average\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Function to save the evaluation results to a CSV file\n",
    "def save_evaluation_results(results_df, output_file):\n",
    "    results_df.to_csv(output_file, index=True)\n",
    "    print(f\"Evaluation results saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'data/training/metricSolution'\n",
    "output_file = 'data/training/metricEvaluation/submetricEvaluation.csv'\n",
    "\n",
    "# Run the evaluation on all files in the directory\n",
    "results_df = evaluate_files(input_dir)\n",
    "\n",
    "# Save the results to CSV\n",
    "save_evaluation_results(results_df, output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade94f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
