{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86e48b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.2\n",
      "TensorFlow version: 2.17.0\n",
      "NumPy version: 1.23.5\n",
      "scikit-learn version: 1.4.2\n",
      "Transformers version: 4.44.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "\n",
    "# Required Libraries and Their Purposes:\n",
    "# - pandas: Used for data manipulation and analysis, particularly for reading and writing CSV files.\n",
    "# - transformers: Provides the BERT model and tokenizer for sequence classification tasks.\n",
    "# - tensorflow: Used for model training, including defining, compiling, and fitting neural networks.\n",
    "# - numpy: Supports numerical operations, such as manipulating arrays and tensors.\n",
    "# - sklearn: Used for splitting data into training and validation sets.\n",
    "\n",
    "# Check and Print Library Versions\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import transformers\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, TFBertModel\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"scikit-learn version:\", sklearn.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "# Required pip installs for the project (Uncomment to install)\n",
    "\n",
    "# !pip install pandas==2.2.2\n",
    "# !pip install tensorflow==2.17.0\n",
    "# !pip install numpy==1.23.5\n",
    "# !pip install scikit-learn==1.4.2\n",
    "# !pip install transformers==4.44.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a8e56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulw\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: data/training/predictionTraining\\prediction1.csv\n",
      "Loading file: data/training/predictionTraining\\prediction2.csv\n",
      "Loading file: data/training/predictionTraining\\prediction3.csv\n",
      "Loading file: data/training/predictionTraining\\prediction4.csv\n",
      "Loading file: data/training/predictionTraining\\prediction5.csv\n",
      "Loading file: data/training/predictionTraining\\prediction6.csv\n",
      "All files concatenated into one DataFrame.\n",
      "Tokenization completed successfully.\n",
      "Loading existing model from fine_tuned_bert\\saved_model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at fine_tuned_bert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned BERT model loaded from fine_tuned_bert.\n",
      "Model compiled successfully.\n",
      "Starting model training...\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['bert/bert/pooler/dense/kernel:0', 'bert/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 125s 8s/step - loss: 0.2920 - category_output_loss: 0.1393 - quality_output_loss: 0.1395 - sentiment_output_loss: 0.0132 - category_output_accuracy: 0.9540 - quality_output_accuracy: 0.9713 - sentiment_output_mae: 0.0908 - val_loss: 1.6413 - val_category_output_loss: 0.6020 - val_quality_output_loss: 1.0218 - val_sentiment_output_loss: 0.0175 - val_category_output_accuracy: 0.7500 - val_quality_output_accuracy: 0.7273 - val_sentiment_output_mae: 0.1014\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 88s 8s/step - loss: 0.1870 - category_output_loss: 0.0735 - quality_output_loss: 0.1024 - sentiment_output_loss: 0.0111 - category_output_accuracy: 0.9828 - quality_output_accuracy: 0.9598 - sentiment_output_mae: 0.0814 - val_loss: 1.4817 - val_category_output_loss: 0.5654 - val_quality_output_loss: 0.9034 - val_sentiment_output_loss: 0.0129 - val_category_output_accuracy: 0.8182 - val_quality_output_accuracy: 0.7273 - val_sentiment_output_mae: 0.0898\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 83s 8s/step - loss: 0.1403 - category_output_loss: 0.0577 - quality_output_loss: 0.0704 - sentiment_output_loss: 0.0122 - category_output_accuracy: 0.9770 - quality_output_accuracy: 0.9770 - sentiment_output_mae: 0.0823 - val_loss: 1.7746 - val_category_output_loss: 0.7437 - val_quality_output_loss: 1.0150 - val_sentiment_output_loss: 0.0159 - val_category_output_accuracy: 0.8182 - val_quality_output_accuracy: 0.7045 - val_sentiment_output_mae: 0.0948\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 83s 8s/step - loss: 0.1529 - category_output_loss: 0.0397 - quality_output_loss: 0.1015 - sentiment_output_loss: 0.0117 - category_output_accuracy: 0.9885 - quality_output_accuracy: 0.9598 - sentiment_output_mae: 0.0862 - val_loss: 1.7502 - val_category_output_loss: 0.6776 - val_quality_output_loss: 1.0583 - val_sentiment_output_loss: 0.0143 - val_category_output_accuracy: 0.8182 - val_quality_output_accuracy: 0.7273 - val_sentiment_output_mae: 0.0945\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 61s 6s/step - loss: 0.0931 - category_output_loss: 0.0304 - quality_output_loss: 0.0548 - sentiment_output_loss: 0.0078 - category_output_accuracy: 0.9885 - quality_output_accuracy: 0.9828 - sentiment_output_mae: 0.0707 - val_loss: 1.8122 - val_category_output_loss: 0.6847 - val_quality_output_loss: 1.1133 - val_sentiment_output_loss: 0.0142 - val_category_output_accuracy: 0.8409 - val_quality_output_accuracy: 0.7045 - val_sentiment_output_mae: 0.0937\n",
      "Epoch 5: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Model training completed.\n",
      "Saving the entire model to fine_tuned_bert\\saved_model...\n",
      "INFO:tensorflow:Assets written to: fine_tuned_bert\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fine_tuned_bert\\saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n",
      "Saving the fine-tuned BERT model and tokenizer...\n",
      "Fine-tuned BERT model and tokenizer saved.\n",
      "Tokenization completed successfully.\n",
      "7/7 [==============================] - 22s 3s/step\n",
      "Predictions saved to data/training/predictionSolution\\predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Training\n",
    "\n",
    "# Description:\n",
    "# Trains a multitask BERT-based model to evaluate customer interactions by \n",
    "# predicting categories, quality levels, and sentiment scores from text data. \n",
    "# It includes functions for data preprocessing, such as concatenating multiple \n",
    "# CSV files, tokenizing text inputs with a BERT tokenizer, and mapping \n",
    "# labels to numerical values. The create_multitask_model function constructs a \n",
    "# TensorFlow Keras model with separate output layers for category classification, \n",
    "# quality assessment, and sentiment regression. The train_and_save_model function \n",
    "# orchestrates the workflow by loading and preparing the data, splitting it into \n",
    "# training and validation sets, training the model with early stopping, saving \n",
    "# the trained model and tokenizer, and generating predictions that are saved to \n",
    "# a CSV file for further analysis.\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from transformers import BertTokenizer, TFBertModel\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to scale sentiment output\n",
    "def scale_sentiment_output(x):\n",
    "    return 0.1 + 0.8 * x\n",
    "\n",
    "# Function to create the model using Functional API\n",
    "def create_multitask_model(bert_model, max_length=128):\n",
    "    # Define Inputs\n",
    "    input_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Obtain BERT outputs\n",
    "    bert_output = bert_model([input_ids, attention_mask])[0]  # Last hidden state\n",
    "    cls_token = bert_output[:, 0, :]  # CLS token\n",
    "\n",
    "    # Define Outputs\n",
    "    category_output = tf.keras.layers.Dense(4, activation='softmax', name='category_output')(cls_token)\n",
    "    quality_output = tf.keras.layers.Dense(3, activation='softmax', name='quality_output')(cls_token)\n",
    "    sentiment_linear = tf.keras.layers.Dense(1, activation='sigmoid', name='sentiment_linear')(cls_token)\n",
    "    sentiment_output = tf.keras.layers.Lambda(scale_sentiment_output, name='sentiment_output')(sentiment_linear)\n",
    "\n",
    "    # Create Model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_ids, attention_mask],\n",
    "        outputs={\n",
    "            'category_output': category_output,\n",
    "            'quality_output': quality_output,\n",
    "            'sentiment_output': sentiment_output\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to concatenate all files into a single DataFrame\n",
    "def concatenate_files(file_paths):\n",
    "    data_frames = []\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Loading file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_frames.append(df)\n",
    "    concatenated_df = pd.concat(data_frames, ignore_index=True)\n",
    "    return concatenated_df\n",
    "\n",
    "# Function to tokenize texts\n",
    "def tokenize_texts(tokenizer, texts, max_length=128):\n",
    "    try:\n",
    "        inputs = tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"tf\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "        print(\"Tokenization completed successfully.\")\n",
    "        return inputs\n",
    "    except Exception as e:\n",
    "        print(f\"Error during tokenization: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to map labels\n",
    "def map_labels(data):\n",
    "    category_mapping = {\n",
    "        'Greetings': 0,\n",
    "        'Problem Investigation': 1,\n",
    "        'Closure': 2,\n",
    "        'Account Verification': 3\n",
    "    }\n",
    "    quality_mapping = {\n",
    "        'Positive': 0,\n",
    "        'Neutral': 1,\n",
    "        'Negative': 2\n",
    "    }\n",
    "    data['Category Label'] = data['Category Truth'].map(category_mapping)\n",
    "    data['Quality Label'] = data['Quality Truth'].map(quality_mapping)\n",
    "    data['Sentiment Label'] = data['Sentiment Truth'].astype(float)\n",
    "    return data\n",
    "\n",
    "# Function to make predictions and save to CSV, including truth columns\n",
    "def predict_and_save(model, tokenizer, data, output_dir):\n",
    "    texts = data['Text']\n",
    "    inputs = tokenize_texts(tokenizer, texts)\n",
    "\n",
    "    predictions = model.predict([inputs['input_ids'], inputs['attention_mask']])\n",
    "\n",
    "    # Extract predictions\n",
    "    predicted_categories = np.argmax(predictions['category_output'], axis=1)\n",
    "    predicted_qualities = np.argmax(predictions['quality_output'], axis=1)\n",
    "    predicted_sentiments = predictions['sentiment_output'].flatten()\n",
    "\n",
    "    # Convert predicted integer labels back to their respective categories\n",
    "    inverse_category_mapping = {0: 'Greetings', 1: 'Problem Investigation', 2: 'Closure', 3: 'Account Verification'}\n",
    "    inverse_quality_mapping = {0: 'Positive', 1: 'Neutral', 2: 'Negative'}\n",
    "\n",
    "    # Map predictions to their respective labels\n",
    "    data['Predicted Category'] = pd.Series(predicted_categories).map(inverse_category_mapping)\n",
    "    data['Predicted Quality'] = pd.Series(predicted_qualities).map(inverse_quality_mapping)\n",
    "    data['Predicted Sentiment'] = predicted_sentiments.round(2)  # Rounded for readability\n",
    "\n",
    "    # Save the updated DataFrame with predictions, including the original truth columns\n",
    "    output_file = os.path.join(output_dir, 'predictions.csv')\n",
    "    data[['Person', 'Text', 'Category Truth', 'Quality Truth', 'Sentiment Truth',\n",
    "          'Predicted Category', 'Predicted Quality', 'Predicted Sentiment']].to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "# Function to train and save the model using Functional API\n",
    "def train_and_save_model():\n",
    "    # Define your input and output directories for CSV files\n",
    "    input_dir = 'data/training/predictionTraining'\n",
    "    output_dir = 'data/training/predictionSolution'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Directory for saving the model and tokenizer\n",
    "    bert_dir = 'fine_tuned_bert'\n",
    "    os.makedirs(bert_dir, exist_ok=True)\n",
    "\n",
    "    # Define model path\n",
    "    model_path = os.path.join(bert_dir, 'saved_model')\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Get list of all CSV files in the input directory\n",
    "    file_paths = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"No CSV files found in the directory: {input_dir}\")\n",
    "        return\n",
    "\n",
    "    # Concatenate all files into a single DataFrame\n",
    "    data = concatenate_files(file_paths)\n",
    "    print(\"All files concatenated into one DataFrame.\")\n",
    "\n",
    "    # Map labels\n",
    "    data = map_labels(data)\n",
    "\n",
    "    # Prepare texts and labels\n",
    "    texts = data['Text']\n",
    "    labels_category = data['Category Label'].values\n",
    "    labels_quality = data['Quality Label'].values\n",
    "    labels_sentiment = data['Sentiment Label'].values\n",
    "\n",
    "    # Tokenize texts\n",
    "    inputs = tokenize_texts(tokenizer, texts)\n",
    "\n",
    "    # Convert inputs to numpy arrays\n",
    "    input_ids = inputs['input_ids'].numpy()\n",
    "    attention_mask = inputs['attention_mask'].numpy()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        np.arange(len(input_ids)),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=labels_category\n",
    "    )\n",
    "\n",
    "    x_train_ids = input_ids[train_indices]\n",
    "    x_val_ids = input_ids[val_indices]\n",
    "    x_train_mask = attention_mask[train_indices]\n",
    "    x_val_mask = attention_mask[val_indices]\n",
    "\n",
    "    y_train_category = labels_category[train_indices]\n",
    "    y_val_category = labels_category[val_indices]\n",
    "    y_train_quality = labels_quality[train_indices]\n",
    "    y_val_quality = labels_quality[val_indices]\n",
    "    y_train_sentiment = labels_sentiment[train_indices]\n",
    "    y_val_sentiment = labels_sentiment[val_indices]\n",
    "\n",
    "    # Prepare training and validation data\n",
    "    train_dataset = (\n",
    "        (x_train_ids, x_train_mask),\n",
    "        {\n",
    "            'category_output': y_train_category,\n",
    "            'quality_output': y_train_quality,\n",
    "            'sentiment_output': y_train_sentiment\n",
    "        }\n",
    "    )\n",
    "    val_dataset = (\n",
    "        (x_val_ids, x_val_mask),\n",
    "        {\n",
    "            'category_output': y_val_category,\n",
    "            'quality_output': y_val_quality,\n",
    "            'sentiment_output': y_val_sentiment\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Check if model exists\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model from {model_path}...\")\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects={\n",
    "                'TFBertModel': TFBertModel,\n",
    "                'scale_sentiment_output': scale_sentiment_output\n",
    "            }\n",
    "        )\n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "        # Check if 'tf_model.h5' exists in bert_dir\n",
    "        tf_model_h5_path = os.path.join(bert_dir, 'tf_model.h5')\n",
    "        if os.path.exists(tf_model_h5_path):\n",
    "            # Load the fine-tuned BERT model separately\n",
    "            bert_model = TFBertModel.from_pretrained(bert_dir)\n",
    "            print(\"Fine-tuned BERT model loaded from fine_tuned_bert.\")\n",
    "        else:\n",
    "            # Get the bert_model from the loaded model\n",
    "            try:\n",
    "                bert_model = model.get_layer('bert')\n",
    "                print(\"BERT model extracted from loaded model.\")\n",
    "            except ValueError:\n",
    "                # Layer not found, try alternative names\n",
    "                bert_model = None\n",
    "                for layer in model.layers:\n",
    "                    if isinstance(layer, TFBertModel):\n",
    "                        bert_model = layer\n",
    "                        print(f\"BERT model found in model layers: {layer.name}\")\n",
    "                        break\n",
    "                if bert_model is None:\n",
    "                    raise ValueError(\"Could not find TFBertModel in the loaded model.\")\n",
    "    else:\n",
    "        # Create bert_model\n",
    "        bert_model = TFBertModel.from_pretrained('bert-base-uncased', name='bert')\n",
    "        print(\"Base BERT model loaded.\")\n",
    "        # Create the multi-task model using bert_model\n",
    "        model = create_multitask_model(bert_model, max_length=128)\n",
    "        print(\"New model created.\")\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "        loss={\n",
    "            'category_output': 'sparse_categorical_crossentropy',\n",
    "            'quality_output': 'sparse_categorical_crossentropy',\n",
    "            'sentiment_output': 'mean_squared_error'\n",
    "        },\n",
    "        metrics={\n",
    "            'category_output': 'accuracy',\n",
    "            'quality_output': 'accuracy',\n",
    "            'sentiment_output': 'mae'\n",
    "        }\n",
    "    )\n",
    "    print(\"Model compiled successfully.\")\n",
    "\n",
    "    # Define early stopping callback\n",
    "    earlystopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    model.fit(\n",
    "        x=train_dataset[0],\n",
    "        y=train_dataset[1],\n",
    "        validation_data=val_dataset,\n",
    "        epochs=10,  # Adjust epochs as needed\n",
    "        batch_size=16,\n",
    "        callbacks=[earlystopping_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Model training completed.\")\n",
    "\n",
    "    # Save the entire model in TensorFlow's SavedModel format\n",
    "    print(f\"Saving the entire model to {model_path}...\")\n",
    "    model.save(model_path, save_format='tf')\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "    # Save the fine-tuned BERT model and tokenizer\n",
    "    print(\"Saving the fine-tuned BERT model and tokenizer...\")\n",
    "    bert_model.save_pretrained(bert_dir)\n",
    "    tokenizer.save_pretrained(bert_dir)\n",
    "    print(\"Fine-tuned BERT model and tokenizer saved.\")\n",
    "\n",
    "    # Make predictions and save results\n",
    "    predict_and_save(model, tokenizer, data, output_dir)\n",
    "\n",
    "# Execute the training or load existing model and predict\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e641a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data/training/predictionSolution\\predictions.csv\n",
      "Results appended to data/training/evaluations/evaluation.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Combined evaluation with appending logic\n",
    "\n",
    "# Description:\n",
    "# Processes multiple CSV files within the data/training/predictionSolution \n",
    "# directory to evaluate the accuracy of predicted categories, quality levels, \n",
    "# and sentiment scores against their true values. For each file, it calculates\n",
    "# the percentage accuracy for category and quality predictions, as well as \n",
    "# sentiment accuracy based on a specified tolerance level. The results are \n",
    "# aggregated and appended to a consolidated CSV file (evaluation.csv) in the \n",
    "# data/training/evaluations directory, ensuring continuous updates without \n",
    "# overwriting existing data.\n",
    "\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "\n",
    "def calculate_scores(file_path, tolerance=0.1):\n",
    "    \n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure that the necessary columns exist\n",
    "    required_columns = ['Category Truth', 'Predicted Category', \n",
    "                        'Quality Truth', 'Predicted Quality',\n",
    "                        'Sentiment Truth', 'Predicted Sentiment']\n",
    "    for col in required_columns:\n",
    "        if col not in data.columns:\n",
    "            raise KeyError(f\"Column '{col}' is missing from the data.\")\n",
    "    \n",
    "    # Calculate Category Accuracy\n",
    "    category_correct = (data['Predicted Category'] == data['Category Truth']).sum()\n",
    "    total_predictions = len(data)\n",
    "    category_accuracy = (category_correct / total_predictions) * 100\n",
    "    \n",
    "    # Calculate Quality Accuracy\n",
    "    quality_correct = (data['Predicted Quality'] == data['Quality Truth']).sum()\n",
    "    quality_accuracy = (quality_correct / total_predictions) * 100\n",
    "    \n",
    "    # Calculate Sentiment Accuracy\n",
    "    sentiment_within_tolerance = (abs(data['Predicted Sentiment'] - data['Sentiment Truth']) <= tolerance).sum()\n",
    "    sentiment_accuracy = (sentiment_within_tolerance / total_predictions) * 100\n",
    "    \n",
    "    # Prepare the results dictionary\n",
    "    results = {\n",
    "        'file': os.path.basename(file_path),\n",
    "        'category_accuracy (%)': round(category_accuracy, 2),\n",
    "        'quality_accuracy (%)': round(quality_accuracy, 2),\n",
    "        'sentiment_accuracy (%)': round(sentiment_accuracy, 2),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_multiple_files(input_directory, output_file, tolerance=0.1):\n",
    "    \n",
    "    # Find all CSV files in the input directory\n",
    "    file_paths = glob.glob(os.path.join(input_directory, '*.csv'))\n",
    "    \n",
    "    if not file_paths:\n",
    "        print(f\"No CSV files found in the directory: {input_directory}\")\n",
    "        return\n",
    "\n",
    "    # Initialize an empty list to store all evaluation results\n",
    "    all_results = []\n",
    "\n",
    "    for file in file_paths:\n",
    "        print(f\"Processing file: {file}\")\n",
    "        try:\n",
    "            # Calculate scores and append the results to the list\n",
    "            all_results.append(calculate_scores(file, tolerance))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "            continue  # Proceed to the next file\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No evaluation results to save.\")\n",
    "        return\n",
    "\n",
    "    # Convert the list of results into a DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.exists(output_file):\n",
    "        # If the file exists, load it and append the new results\n",
    "        existing_df = pd.read_csv(output_file)\n",
    "        results_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
    "\n",
    "    # Save the results, appending to the existing data\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)  # Ensure the output directory exists\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Results appended to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing evaluation files\n",
    "    input_dir = 'data/training/predictionSolution'\n",
    "    \n",
    "    # Path to save combined evaluation results\n",
    "    output_file = 'data/training/evaluations/evaluation.csv'\n",
    "    \n",
    "    # Process all evaluation files in the directory and save to a single file\n",
    "    process_multiple_files(input_dir, output_file, tolerance=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861b8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
