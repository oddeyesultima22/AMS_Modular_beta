{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e9316e-89fc-487e-8abb-a0546e748736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import pickle\n",
    "import warnings\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "\n",
    "# from resemblyzer import preprocess_wav, VoiceEncoder\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), 'lib'))\n",
    "# from demo_utils import *\n",
    "\n",
    "# os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\"  # depends on where you install ffmpeg, run 'which ffmpeg' to check\n",
    "\n",
    "# from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "\n",
    "# import speech_recognition as sr\n",
    "# !pip install torchaudio pyannote.audio torch\n",
    " # !pip install huggingface_hub\n",
    "import torchaudio\n",
    "from pyannote.audio import Pipeline\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import torch\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847bde8-febc-4c28-8ba7-67d36f2cda63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d79184e-9afe-434b-88c0-b19fe0fc6f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quxch/Documents/DEAKIN/2024T2/SIT764/AMS-Project/diarization_model\n"
     ]
    }
   ],
   "source": [
    "# to get current path\n",
    "path = os.getcwd() \n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6e44e3-6d9e-4ece-b2e0-075cf19d4ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quxch/Documents/DEAKIN/2024T2/SIT764/AMS-Project/diarization_model/data/sampleCall2.mp3\n",
      "/Users/quxch/Documents/DEAKIN/2024T2/SIT764/AMS-Project/diarization_model/data/sampleCall2.wav\n"
     ]
    }
   ],
   "source": [
    "audio_file_name = 'sampleCall2'\n",
    "audio_file_path = path + f'/data/{audio_file_name}.mp3'\n",
    "wav_fpath = path +  f'/data/{audio_file_name}.wav'\n",
    "print(audio_file_path)\n",
    "print(wav_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e0d218-2163-447c-a888-88ed77976e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting\n"
     ]
    }
   ],
   "source": [
    "## Converting mp3 to wav format\n",
    "# Load the audio as a waveform `wav` and Store the sampling rate as `sampling_rate`\n",
    "wav,_ = librosa.load(audio_file_path, sr=16000)\n",
    "sf.write(wav_fpath, wav, _, 'PCM_16')\n",
    "print(\"Done converting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9fbaaa-dcd7-4096-b654-0139d2d71d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e9d3e-2b2c-4eef-ac17-7d79608b4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n",
      " 20%|████████▌                                  | 2/10 [08:15<32:54, 246.79s/it]"
     ]
    }
   ],
   "source": [
    "# instantiate the pipeline\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=token)\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "for i in tqdm(range(10)):\n",
    "    diarization = pipeline(wav_fpath, num_speakers=2)\n",
    "# diarization = pipeline(wav_fpath, min_speakers=2, max_speakers=5)\n",
    "\n",
    "# # dump the diarization output to disk using RTTM format\n",
    "# with open(f\"/data/{audio_file_name}.rttm\", \"w\") as rttm:\n",
    "#     diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e1b0f-6dea-4b28-9c03-14684df5196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    waveform, sample_rate = torchaudio.load(wav_fpath)\n",
    "    diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a8bd33f-cd91-41e7-9757-6edb027f719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0c7e9260234673b455e40107e58ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
    "# with ProgressHook() as hook:\n",
    "#     diarization = pipeline(wav_fpath, num_speakers=2,hook=hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99ff82-74e0-4147-bc7c-97aaa9ac495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wave2Vec 2.0 model and tokenizer for speech recognition\n",
    "model_name = \"facebook/wav2vec2-large-960h\"\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1348730-2d5c-4a29-ad91-33bab1a36ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  transcribe audio segment and calculate confidence\n",
    "def transcribe_segment(segment_waveform):\n",
    "    input_values = tokenizer(segment_waveform.squeeze().numpy(), return_tensors=\"pt\").input_values\n",
    "    logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    \n",
    "    # Calculate confidence\n",
    "    softmax = F.softmax(logits, dim=-1)\n",
    "    max_probs, _ = torch.max(softmax, dim=-1)\n",
    "    confidence = max_probs.mean().item()\n",
    "    \n",
    "    return transcription, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540631a-beab-481d-9b44-ad5c658908a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(10)):\n",
    "    transcriptions = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        start = int(turn.start * sample_rate)\n",
    "        end = int(turn.end * sample_rate)\n",
    "        segment_waveform = waveform[:, start:end]\n",
    "        \n",
    "        transcription, confidence = transcribe_segment(segment_waveform)\n",
    "        transcriptions.append((turn.start, turn.end, speaker, transcription, confidence))\n",
    "\n",
    "# Display the transcriptions with confidence scores\n",
    "df = pd.DataFrame(transcriptions, columns=[\"Start Time\", \"End Time\", \"Speaker\", \"Transcription\", \"Confidence\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc67846-9bbc-4b93-8062-3457e7c9a200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fcd13-0650-45b2-9426-2b486949b0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50b5aa-5cc4-4ac4-9ba5-c4bbb32c6c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a7496-99fa-44e7-b51b-344b5491a331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d1bbb-3ea1-475b-9e58-76891e6c1986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbd16a-e67e-4046-b4e8-0149c53e7463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
